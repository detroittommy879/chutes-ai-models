# Token Test Feature - Quick Start Guide

## ğŸ¯ What Is This?

The **Token Test** feature tests AI models with LARGE inputs (thousands to hundreds of thousands of tokens) to see how they perform when pushed to their limits.

## ğŸš€ How to Use

### Step 1: Load the App
```bash
cd "C:\2nd\Main\Git-Projects\z_aiimage\chutes-models-enhanced - Copy"
node server.js
```

Open: http://localhost:3888

### Step 2: Select Models
- Scroll down to the model table
- Check the boxes next to models you want to test
- (Only VLLM and TGI models will be tested)

### Step 3: Run Token Test
- Click the **"Run Token Test"** button (orange/accent colored)
- Wait for tests to complete
- View results below

## ğŸ“Š What You'll See

### Result Display:
```
âœ… SUCCESS                    1234ms
ğŸ“„ Token Test: Used tokens_250,000.txt | Model Context: 262,144
Token Usage: Input: 250041 | Output: 42 | Total: 250083
Response: [Model's output here]
```

### For Each Model:
- âœ… or âŒ Status
- Response time in milliseconds
- Token file used (automatically selected)
- Model's max input context
- Actual token usage (input/output/total)
- Response preview

## ğŸ¯ Token File Selection Logic

The app automatically picks the right size file:

```
Model Context     â†’  Token File Used
256k (262,144)   â†’  tokens_250,000.txt (250k tokens)
128k (131,072)   â†’  tokens_125,000.txt (125k tokens)
64k (65,536)     â†’  tokens_61,000.txt  (61k tokens)
32k (32,768)     â†’  tokens_30,000.txt  (30k tokens)
16k (16,384)     â†’  tokens_14,000.txt  (14k tokens)
8k (8,192)       â†’  tokens_7,000.txt   (7k tokens)
```

**Rule**: Always use the **largest token file that's smaller** than the model's max context.

## ğŸ” Examples

### Example 1: Testing GPT-4 Class Model
- Model: `gpt-4-turbo` (128k context)
- Selected File: `tokens_125,000.txt` (125k tokens)
- Result: Shows how fast it processes ~125k tokens

### Example 2: Testing Smaller Model  
- Model: `llama-3-8b` (8k context)
- Selected File: `tokens_7,000.txt` (7k tokens)
- Result: Tests performance near its limit

## âš¡ Regular Test vs Token Test

### Regular Test Button (Blue)
- Uses text from the prompt box
- Small input (~10-100 tokens)
- Quick test (1-5 seconds)
- **Use for**: Quick checks

### Token Test Button (Orange)
- Uses large token files
- Large input (7k-250k tokens)
- Slower test (10-60+ seconds)
- **Use for**: Stress testing, performance benchmarks

## ğŸ’¡ Pro Tips

1. **Start Small**: Test with 1-2 models first
2. **Check Context**: Models with larger contexts = longer tests
3. **Monitor Console**: Look for log messages in browser DevTools
4. **Be Patient**: Large context tests take time
5. **Watch Costs**: Large context tests use more API credits

## ğŸ› Troubleshooting

### "No suitable token file found"
- Model doesn't have context length info
- Need to create smaller token files

### "Model has no input context information"
- Model data missing from v1 API
- Try selecting a different model

### Tests taking too long
- Normal for large contexts (250k tokens can take 30-60+ seconds)
- Check server logs for progress

### Button disabled
- Select at least one VLLM or TGI model
- Wait for models to finish loading

## ğŸ“ Token Files Location

```
bigtokens/generated_tokens/
â”œâ”€â”€ tokens_7,000.txt      (7k tokens,  36 KB)
â”œâ”€â”€ tokens_8,000.txt      (8k tokens,  41 KB)
â”œâ”€â”€ tokens_14,000.txt     (14k tokens, 72 KB)
â”œâ”€â”€ tokens_30,000.txt     (30k tokens, 155 KB)
â”œâ”€â”€ tokens_32,000.txt     (32k tokens, 165 KB)
â”œâ”€â”€ tokens_61,000.txt     (61k tokens, 315 KB)
â”œâ”€â”€ tokens_125,000.txt    (125k tokens, 646 KB)
â””â”€â”€ tokens_250,000.txt    (250k tokens, 1.2 MB)
```

## ğŸ“ Use Cases

### 1. Performance Testing
Test how different models handle large contexts:
```
Model A: 250k tokens â†’ 15,234ms âœ…
Model B: 250k tokens â†’ 45,678ms âœ…
Model C: 250k tokens â†’ TIMEOUT âŒ
```

### 2. Cost Analysis
Compare token usage across models:
```
Model A: 250k input â†’ 100 output tokens
Model B: 250k input â†’ 500 output tokens
```

### 3. Reliability Testing
Find models that fail with large contexts:
```
Model A: âœ… Handles 250k
Model B: âŒ Fails at 125k
Model C: âœ… Handles 250k
```

### 4. Production Readiness
Before using a model in production:
```
âœ… Test with expected input size
âœ… Verify response quality
âœ… Check latency acceptability
âœ… Confirm cost per request
```

## ğŸ”„ Workflow Example

1. **Select 5 models** with 256k context
2. **Click "Run Token Test"**
3. **Watch progress**: Each model tested sequentially
4. **Review results**: Compare latency and reliability
5. **Choose best model** based on speed + reliability + cost

## ğŸ“ˆ Interpreting Results

### Good Result:
```
âœ… SUCCESS                    8,456ms
Input: 125,044 | Output: 156 | Total: 125,200
Response: [Coherent output]
```
â†’ Model handled large context well!

### Concerning Result:
```
âŒ ERROR                     45,678ms
Error: Request timeout
```
â†’ Model struggled with large context

### Mediocre Result:
```
âœ… SUCCESS                    35,234ms
Input: 125,044 | Output: 12 | Total: 125,056
Response: I cannot process this request.
```
â†’ Model processed but didn't handle content properly

## ğŸ‰ You're Ready!

Start testing your models with large contexts and see which ones perform best under pressure!
